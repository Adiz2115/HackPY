{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10364989,"sourceType":"datasetVersion","datasetId":6419745}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gradio\n!pip install faiss-cpu\n!pip install PyMuPDF\n!pip install sentence-transformers\n!pip install torch transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T09:58:21.284275Z","iopub.execute_input":"2025-01-04T09:58:21.284486Z","iopub.status.idle":"2025-01-04T09:58:52.886940Z","shell.execute_reply.started":"2025-01-04T09:58:21.284457Z","shell.execute_reply":"2025-01-04T09:58:52.885813Z"}},"outputs":[{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.5.2 (from gradio)\n  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting httpx>=0.24.1 (from gradio)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting huggingface-hub>=0.25.1 (from gradio)\n  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\nCollecting orjson~=3.0 (from gradio)\n  Downloading orjson-3.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\nCollecting ruff>=0.2.2 (from gradio)\n  Downloading ruff-0.8.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.6.1)\nRequirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\nCollecting httpcore==1.* (from httpx>=0.24.1->gradio)\n  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.8.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, safehttpx, gradio-client, gradio\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.24.7\n    Uninstalling huggingface-hub-0.24.7:\n      Successfully uninstalled huggingface-hub-0.24.7\nSuccessfully installed fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.27.0 orjson-3.10.13 python-multipart-0.0.20 ruff-0.8.5 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\nDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.9.0.post1\nCollecting PyMuPDF\n  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\nDownloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyMuPDF\nSuccessfully installed PyMuPDF-1.25.1\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T09:58:52.891364Z","iopub.execute_input":"2025-01-04T09:58:52.891716Z","iopub.status.idle":"2025-01-04T09:58:57.980213Z","shell.execute_reply.started":"2025-01-04T09:58:52.891683Z","shell.execute_reply":"2025-01-04T09:58:57.979161Z"}},"outputs":[{"name":"stdout","text":"Collecting pdfplumber\n  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\nDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\nSuccessfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# TEXT BOX Interface","metadata":{}},{"cell_type":"code","source":"import gradio as gr\nimport numpy as np\nfrom typing import List, Tuple\nimport faiss\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom sentence_transformers import SentenceTransformer\nimport re\nfrom dataclasses import dataclass\nimport pickle\nimport os\nfrom pathlib import Path\nfrom io import BytesIO\nimport pdfplumber\nimport tempfile\n\n# Configuration\nCACHE_DIR = Path(\"cache\")\nCHUNK_SIZE = 1000\nCHUNK_OVERLAP = 200\nEMBED_DIMENSION = 384  # Dimension for MiniLM embeddings\nTOP_K_MATCHES = 5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n@dataclass\nclass Document:\n    text: str\n    embedding: np.ndarray = None\n\ndef load_models():\n    # Load embedding model\n    embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=DEVICE)\n\n    # Load LLM and tokenizer\n    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n        low_cpu_mem_usage=True\n    ).to(DEVICE)\n\n    return embedding_model, model, tokenizer\n\ndef create_cache_dir():\n    \"\"\"Create cache directory if it doesn't exist\"\"\"\n    CACHE_DIR.mkdir(exist_ok=True)\n\ndef get_cache_path(filename: str) -> Path:\n    \"\"\"Get path for cached embeddings\"\"\"\n    return CACHE_DIR / f\"{filename}.pkl\"\n\ndef extract_text_from_pdf(file_obj) -> str:\n    \"\"\"Extract text from uploaded PDF file using pdfplumber\"\"\"\n    try:\n        # Create a temporary file\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n            # If it's a Gradio file object\n            if hasattr(file_obj, 'name'):\n                with open(file_obj.name, 'rb') as f:\n                    tmp_file.write(f.read())\n            else:\n                tmp_file.write(file_obj.read())\n            tmp_path = tmp_file.name\n\n        text = \"\"\n        # Use pdfplumber to extract text\n        with pdfplumber.open(tmp_path) as pdf:\n            for page in pdf.pages:\n                try:\n                    text += page.extract_text() or \"\"\n                except Exception as e:\n                    print(f\"Error on page: {str(e)}\")\n                    continue\n\n        # Clean up temporary file\n        os.unlink(tmp_path)\n\n        if not text.strip():\n            alternative_text = extract_text_with_backup_method(tmp_path)\n            if alternative_text:\n                return alternative_text\n            return \"Error: No readable text found in the PDF.\"\n\n        return text\n\n    except Exception as e:\n        if 'tmp_path' in locals():\n            os.unlink(tmp_path)\n        return f\"Error extracting text from PDF: {str(e)}\"\n\ndef extract_text_with_backup_method(pdf_path):\n    \"\"\"Backup method using pdf2text if available\"\"\"\n    try:\n        from pdfminer.high_level import extract_text\n        return extract_text(pdf_path)\n    except:\n        try:\n            import textract\n            return textract.process(pdf_path).decode('utf-8')\n        except:\n            return None\n\ndef preprocess_text(text: str) -> str:\n    \"\"\"Clean and preprocess extracted text\"\"\"\n    # Remove excessive whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    # Remove special characters but keep basic punctuation\n    text = re.sub(r'[^\\w\\s.,!?;:-]', '', text)\n    # Remove multiple periods\n    text = re.sub(r'\\.{2,}', '.', text)\n    # Fix spaces around punctuation\n    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n    return text.strip()\n\ndef create_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n    \"\"\"Split text into overlapping chunks\"\"\"\n    chunks = []\n    start = 0\n    text_length = len(text)\n\n    # Handle very short texts\n    if text_length < chunk_size:\n        return [text] if text else []\n\n    while start < text_length:\n        end = start + chunk_size\n\n        if end >= text_length:\n            chunks.append(text[start:])\n            break\n\n        # Find the last period or appropriate breaking point\n        last_period = text.rfind('.', start, end)\n        if last_period != -1 and last_period > start + chunk_size/2:\n            end = last_period + 1\n        else:\n            # If no period found, try to break at a space\n            while end > start and text[end] != ' ':\n                end -= 1\n            if end == start:  # If no space found, force break at chunk_size\n                end = start + chunk_size\n\n        chunk = text[start:end].strip()\n        if chunk:  # Only add non-empty chunks\n            chunks.append(chunk)\n        start = max(start + chunk_size - overlap, end - overlap)\n\n    return chunks\n\ndef get_embeddings(texts: List[str], embedding_model: SentenceTransformer) -> List[np.ndarray]:\n    \"\"\"Generate embeddings using sentence-transformers\"\"\"\n    try:\n        embeddings = embedding_model.encode(texts, show_progress_bar=False)\n        return [np.array(embedding) for embedding in embeddings]\n    except Exception as e:\n        print(f\"Error generating embeddings: {str(e)}\")\n        return None\n\ndef create_faiss_index(embeddings: List[np.ndarray]) -> faiss.IndexFlatL2:\n    \"\"\"Create and populate FAISS index\"\"\"\n    embeddings_array = np.array(embeddings).astype('float32')\n    index = faiss.IndexFlatL2(EMBED_DIMENSION)\n    index.add(embeddings_array)\n    return index\n\ndef get_relevant_chunks(query: str, chunks: List[str], faiss_index: faiss.IndexFlatL2, embedding_model: SentenceTransformer) -> List[str]:\n    \"\"\"Retrieve most relevant chunks for the query\"\"\"\n    query_embedding = embedding_model.encode([query])[0]\n    D, I = faiss_index.search(\n        np.array([query_embedding]).astype('float32'),\n        min(TOP_K_MATCHES, len(chunks))\n    )\n    return [chunks[i] for i in I[0]]\n\ndef generate_answer(query: str, context: List[str], model, tokenizer) -> str:\n    \"\"\"Generate answer using TinyLlama\"\"\"\n    try:\n        # Prepare prompt\n        context_text = \"\\n\".join(context)\n        prompt = f\"\"\"<|system|>\nYou are a helpful assistant. Answer the question based only on the provided context. If the answer cannot be found in the context, say so.\n\nContext:\n{context_text}\n\n<|user|>\n{query}\n\n<|assistant|>\"\"\"\n\n        # Generate response\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=2048,\n            num_return_sequences=1,\n            temperature=0.6,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        try:\n            response = response.split(\"<|assistant|>\")[-1].strip()\n        except:\n            response = response.strip()\n\n        return response\n    except Exception as e:\n        return f\"Error generating answer: {str(e)}\"\n\nclass PDFQuestionAnswering:\n    def __init__(self):\n        create_cache_dir()\n        print(f\"Using device: {DEVICE}\")\n        self.embedding_model, self.llm_model, self.tokenizer = load_models()\n        self.current_chunks = None\n        self.current_faiss_index = None\n\n    def process_pdf(self, pdf_file):\n        if pdf_file is None:\n            return \"Please upload a PDF file.\"\n\n        try:\n            # Check cache for processed embeddings\n            cache_path = get_cache_path(pdf_file.name)\n\n            if cache_path.exists():\n                print(\"Loading from cache...\")\n                with cache_path.open('rb') as f:\n                    cached_data = pickle.load(f)\n                    self.current_chunks = cached_data['chunks']\n                    self.current_faiss_index = cached_data['faiss_index']\n                return \"PDF loaded from cache! You can now ask questions.\"\n\n            print(\"Processing new PDF...\")\n            # Process PDF\n            text = extract_text_from_pdf(pdf_file)\n            if isinstance(text, str) and not text.startswith(\"Error\"):\n                print(\"Text extracted successfully, preprocessing...\")\n                processed_text = preprocess_text(text)\n                if not processed_text:\n                    return \"No readable text found in the PDF.\"\n\n                print(\"Creating chunks...\")\n                self.current_chunks = create_chunks(processed_text)\n\n                if not self.current_chunks:\n                    return \"No valid text chunks could be created from the PDF.\"\n\n                # Generate embeddings\n                print(\"Generating embeddings...\")\n                embeddings = get_embeddings(self.current_chunks, self.embedding_model)\n                if embeddings:\n                    # Create FAISS index\n                    print(\"Creating FAISS index...\")\n                    self.current_faiss_index = create_faiss_index(embeddings)\n\n                    # Cache the processed data\n                    print(\"Caching results...\")\n                    with cache_path.open('wb') as f:\n                        pickle.dump({\n                            'chunks': self.current_chunks,\n                            'faiss_index': self.current_faiss_index\n                        }, f)\n\n                    return f\"PDF processed successfully! Extracted {len(self.current_chunks)} chunks of text. You can now ask questions.\"\n                else:\n                    return \"Failed to generate embeddings\"\n            else:\n                return text  # Return error message\n        except Exception as e:\n            return f\"Error processing PDF: {str(e)}\"\n\n    def answer_question(self, question):\n        if self.current_chunks is None or self.current_faiss_index is None:\n            return \"Please upload and process a PDF first.\"\n\n        if not question.strip():\n            return \"Please enter a question.\"\n\n        try:\n            # Get relevant chunks\n            print(\"Finding relevant chunks...\")\n            relevant_chunks = get_relevant_chunks(\n                question,\n                self.current_chunks,\n                self.current_faiss_index,\n                self.embedding_model\n            )\n\n            if not relevant_chunks:\n                return \"Could not find relevant context in the document.\"\n\n            # Generate answer\n            print(\"Generating answer...\")\n            answer = generate_answer(\n                question,\n                relevant_chunks,\n                self.llm_model,\n                self.tokenizer\n            )\n\n            # Format response with relevant chunks\n            response = f\"Answer: {answer}\\n\\nRelevant Context:\\n\"\n            for i, chunk in enumerate(relevant_chunks, 1):\n                response += f\"\\nChunk {i}:\\n{chunk}\\n---\"\n\n            return response\n        except Exception as e:\n            return f\"Error answering question: {str(e)}\"\n\ndef create_gradio_interface():\n    qa_system = PDFQuestionAnswering()\n\n    with gr.Blocks(title=\"PDF Question-Answering with RAG\") as interface:\n        gr.Markdown(\"# ğŸ“š PDF Question-Answering with RAG\")\n        gr.Markdown(\"Upload a PDF document and ask questions about its content!\")\n\n        with gr.Row():\n            with gr.Column():\n                pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n                process_button = gr.Button(\"Process PDF\")\n                status_output = gr.Textbox(label=\"Status\", lines=2)\n\n            with gr.Column():\n                question_input = gr.Textbox(label=\"Ask a question about the document\")\n                answer_button = gr.Button(\"Get Answer\")\n                answer_output = gr.Textbox(label=\"Response\", lines=10)\n\n        process_button.click(\n            fn=qa_system.process_pdf,\n            inputs=[pdf_input],\n            outputs=[status_output]\n        )\n\n        answer_button.click(\n            fn=qa_system.answer_question,\n            inputs=[question_input],\n            outputs=[answer_output]\n        )\n\n    return interface\n\n# For Google Colab, add these installation commands at the top of your notebook:\n\"\"\"\n!pip install -q gradio faiss-cpu pdfplumber sentence-transformers torch transformers pdfminer.six textract\n\"\"\"\n\nif __name__ == \"__main__\":\n    demo = create_gradio_interface()\n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T10:15:43.488607Z","iopub.execute_input":"2025-01-04T10:15:43.488997Z","iopub.status.idle":"2025-01-04T10:15:48.673715Z","shell.execute_reply.started":"2025-01-04T10:15:43.488970Z","shell.execute_reply":"2025-01-04T10:15:48.672996Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7862\n* Running on public URL: https://b614201686ecf23ea8.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://b614201686ecf23ea8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Finding relevant chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dba5fbd2e5e4c40836d9b85a511bff9"}},"metadata":{}},{"name":"stdout","text":"Generating answer...\nFinding relevant chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8092fa6343b34602abc5f1e4d4648eae"}},"metadata":{}},{"name":"stdout","text":"Generating answer...\nFinding relevant chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db6f429cf99471ca97ed81f05532a86"}},"metadata":{}},{"name":"stdout","text":"Generating answer...\nFinding relevant chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78dc6e8209cf40ed9751d3aec888109e"}},"metadata":{}},{"name":"stdout","text":"Generating answer...\nFinding relevant chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31787079420744cbb47046988e846df8"}},"metadata":{}},{"name":"stdout","text":"Generating answer...\nProcessing new PDF...\nText extracted successfully, preprocessing...\nCreating chunks...\nGenerating embeddings...\nCreating FAISS index...\nCaching results...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# CHATBOT Interface","metadata":{}},{"cell_type":"code","source":"import gradio as gr\nimport numpy as np\nfrom typing import List, Tuple\nimport faiss\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom sentence_transformers import SentenceTransformer\nimport re\nfrom dataclasses import dataclass\nimport pickle\nimport os\nfrom pathlib import Path\nfrom io import BytesIO\nimport pdfplumber\nimport tempfile\n\n# Configuration\nCACHE_DIR = Path(\"cache\")\nCHUNK_SIZE = 1000\nCHUNK_OVERLAP = 200\nEMBED_DIMENSION = 384\nTOP_K_MATCHES = 5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n@dataclass\nclass Document:\n    text: str\n    embedding: np.ndarray = None\n\ndef load_models():\n    embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=DEVICE)\n    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n        low_cpu_mem_usage=True\n    ).to(DEVICE)\n    return embedding_model, model, tokenizer\n\ndef create_cache_dir():\n    CACHE_DIR.mkdir(exist_ok=True)\n\ndef get_cache_path(filename: str) -> Path:\n    return CACHE_DIR / f\"{filename}.pkl\"\n\ndef extract_text_from_pdf(file_obj) -> str:\n    try:\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n            if hasattr(file_obj, 'name'):\n                with open(file_obj.name, 'rb') as f:\n                    tmp_file.write(f.read())\n            else:\n                tmp_file.write(file_obj.read())\n            tmp_path = tmp_file.name\n\n        text = \"\"\n        with pdfplumber.open(tmp_path) as pdf:\n            for page in pdf.pages:\n                try:\n                    text += page.extract_text() or \"\"\n                except Exception as e:\n                    print(f\"Error on page: {str(e)}\")\n                    continue\n\n        os.unlink(tmp_path)\n\n        if not text.strip():\n            return \"Error: No readable text found in the PDF.\"\n\n        return text\n\n    except Exception as e:\n        if 'tmp_path' in locals():\n            os.unlink(tmp_path)\n        return f\"Error extracting text from PDF: {str(e)}\"\n\ndef preprocess_text(text: str) -> str:\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'[^\\w\\s.,!?;:-]', '', text)\n    text = re.sub(r'\\.{2,}', '.', text)\n    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n    return text.strip()\n\ndef create_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n    chunks = []\n    start = 0\n    text_length = len(text)\n\n    if text_length < chunk_size:\n        return [text] if text else []\n\n    while start < text_length:\n        end = start + chunk_size\n\n        if end >= text_length:\n            chunks.append(text[start:])\n            break\n\n        last_period = text.rfind('.', start, end)\n        if last_period != -1 and last_period > start + chunk_size/2:\n            end = last_period + 1\n        else:\n            while end > start and text[end] != ' ':\n                end -= 1\n            if end == start:\n                end = start + chunk_size\n\n        chunk = text[start:end].strip()\n        if chunk:\n            chunks.append(chunk)\n        start = max(start + chunk_size - overlap, end - overlap)\n\n    return chunks\n\ndef get_embeddings(texts: List[str], embedding_model: SentenceTransformer) -> List[np.ndarray]:\n    try:\n        embeddings = embedding_model.encode(texts, show_progress_bar=False)\n        return [np.array(embedding) for embedding in embeddings]\n    except Exception as e:\n        print(f\"Error generating embeddings: {str(e)}\")\n        return None\n\ndef create_faiss_index(embeddings: List[np.ndarray]) -> faiss.IndexFlatL2:\n    embeddings_array = np.array(embeddings).astype('float32')\n    index = faiss.IndexFlatL2(EMBED_DIMENSION)\n    index.add(embeddings_array)\n    return index\n\ndef get_relevant_chunks(query: str, chunks: List[str], faiss_index: faiss.IndexFlatL2, embedding_model: SentenceTransformer) -> List[str]:\n    query_embedding = embedding_model.encode([query])[0]\n    D, I = faiss_index.search(\n        np.array([query_embedding]).astype('float32'),\n        min(TOP_K_MATCHES, len(chunks))\n    )\n    return [chunks[i] for i in I[0]]\n\ndef generate_answer(query: str, context: List[str], model, tokenizer) -> str:\n    try:\n        context_text = \"\\n\".join(context)\n        prompt = f\"\"\"<|system|>\nYou are a helpful assistant. Answer the question based only on the provided context. If the answer cannot be found in the context, say so.\n\nContext:\n{context_text}\n\n<|user|>\n{query}\n\n<|assistant|>\"\"\"\n\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=2048,\n            num_return_sequences=1,\n            temperature=0.6,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        try:\n            response = response.split(\"<|assistant|>\")[-1].strip()\n        except:\n            response = response.strip()\n\n        return response\n    except Exception as e:\n        return f\"Error generating answer: {str(e)}\"\n\nclass PDFQuestionAnswering:\n    def __init__(self):\n        create_cache_dir()\n        print(f\"Using device: {DEVICE}\")\n        self.embedding_model, self.llm_model, self.tokenizer = load_models()\n        self.current_chunks = None\n        self.current_faiss_index = None\n        self.chat_history = []\n\n    def process_pdf(self, pdf_file):\n        if pdf_file is None:\n            return [], \"Please upload a PDF file.\"\n\n        try:\n            cache_path = get_cache_path(pdf_file.name)\n\n            if cache_path.exists():\n                print(\"Loading from cache...\")\n                with cache_path.open('rb') as f:\n                    cached_data = pickle.load(f)\n                    self.current_chunks = cached_data['chunks']\n                    self.current_faiss_index = cached_data['faiss_index']\n                return [], \"PDF loaded from cache! You can now ask questions about the document.\"\n\n            print(\"Processing new PDF...\")\n            text = extract_text_from_pdf(pdf_file)\n            if isinstance(text, str) and not text.startswith(\"Error\"):\n                processed_text = preprocess_text(text)\n                if not processed_text:\n                    return [], \"No readable text found in the PDF.\"\n\n                self.current_chunks = create_chunks(processed_text)\n                if not self.current_chunks:\n                    return [], \"No valid text chunks could be created from the PDF.\"\n\n                embeddings = get_embeddings(self.current_chunks, self.embedding_model)\n                if embeddings:\n                    self.current_faiss_index = create_faiss_index(embeddings)\n\n                    with cache_path.open('wb') as f:\n                        pickle.dump({\n                            'chunks': self.current_chunks,\n                            'faiss_index': self.current_faiss_index\n                        }, f)\n\n                    return [], f\"PDF processed successfully! You can now ask questions about the document.\"\n                else:\n                    return [], \"Failed to generate embeddings\"\n            else:\n                return [], text\n        except Exception as e:\n            return [], f\"Error processing PDF: {str(e)}\"\n\n    def chat(self, message, history):\n        if self.current_chunks is None or self.current_faiss_index is None:\n            return \"Please upload and process a PDF first.\"\n\n        if not message.strip():\n            return \"Please enter a question.\"\n\n        try:\n            relevant_chunks = get_relevant_chunks(\n                message,\n                self.current_chunks,\n                self.current_faiss_index,\n                self.embedding_model\n            )\n\n            if not relevant_chunks:\n                return \"I couldn't find relevant information in the document to answer your question.\"\n\n            answer = generate_answer(\n                message,\n                relevant_chunks,\n                self.llm_model,\n                self.tokenizer\n            )\n\n            source_context = \"\\n\\nğŸ” *Source Context*:\\n\"\n            for i, chunk in enumerate(relevant_chunks, 1):\n                source_context += f\"\\n{chunk}\\n---\"\n\n            return answer + source_context\n        except Exception as e:\n            return f\"Error answering question: {str(e)}\"\n\ndef create_gradio_interface():\n    qa_system = PDFQuestionAnswering()\n\n    with gr.Blocks(title=\"PDF Chat with RAG\") as interface:\n        gr.Markdown(\"# ğŸ“š Chat with your PDF\")\n        gr.Markdown(\"Upload a PDF document and start a conversation about its content!\")\n\n        with gr.Row():\n            with gr.Column(scale=1):\n                pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n                process_button = gr.Button(\"Process PDF\")\n            \n            with gr.Column(scale=3):\n                chatbot = gr.Chatbot(height=450)\n                message = gr.Textbox(\n                    label=\"Ask a question about the document\",\n                    placeholder=\"Type your question here...\",\n                    lines=2\n                )\n                with gr.Row():\n                    submit = gr.Button(\"Send\")\n                    clear = gr.Button(\"Clear Chat\")\n\n        def respond(message, history):\n            bot_message = qa_system.chat(message, history)\n            history.append((message, bot_message))\n            return \"\", history\n\n        def clear_chat():\n            return None, None\n\n        process_button.click(\n            fn=qa_system.process_pdf,\n            inputs=[pdf_input],\n            outputs=[chatbot, message]\n        )\n\n        submit.click(\n            fn=respond,\n            inputs=[message, chatbot],\n            outputs=[message, chatbot]\n        )\n        message.submit(\n            fn=respond,\n            inputs=[message, chatbot],\n            outputs=[message, chatbot]\n        )\n        clear.click(\n            fn=clear_chat,\n            outputs=[message, chatbot],\n        )\n\n    return interface\n\nif __name__ == \"__main__\":\n    demo = create_gradio_interface()\n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T10:30:21.070246Z","iopub.execute_input":"2025-01-04T10:30:21.070885Z","iopub.status.idle":"2025-01-04T10:30:32.491101Z","shell.execute_reply.started":"2025-01-04T10:30:21.070849Z","shell.execute_reply":"2025-01-04T10:30:32.490195Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7863\n* Running on public URL: https://0e7338b92c6a615e0c.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://0e7338b92c6a615e0c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Loading from cache...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e69df484d604ce9b860113fe34c0349"}},"metadata":{}},{"name":"stderr","text":"This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa238a46e1342f6a6c9c4c0df4f6d6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d074111198dc4d1d89b4068866e18b02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28111f387c684fbb9e3d4f68572c8dee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7092c5fd43c24513bf644bf3af99d660"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}